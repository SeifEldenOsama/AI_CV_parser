{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13697670,"sourceType":"datasetVersion","datasetId":8712999},{"sourceId":13908716,"sourceType":"datasetVersion","datasetId":8861995}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install langchain langchain-community langchain-core transformers==4.52.4 pydantic torch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom langchain.output_parsers import StructuredOutputParser, ResponseSchema\nfrom langchain.prompts import PromptTemplate\nimport torch\nimport re\n\nmodel_name = \"mistralai/Mistral-Nemo-Instruct-2407\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n\ndef generate_text(prompt, max_length=5000, num_return_sequences=1):\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    outputs = model.generate(\n        **inputs,\n        max_length=max_length,\n        num_return_sequences=num_return_sequences,\n        do_sample=True,\n        top_k=50,\n        top_p=0.95,\n        temperature=0.7,\n    )\n    return [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pypdf import PdfReader\nreader = PdfReader(\"/kaggle/input/resume/resume.pdf\")\ntext = \"\".join(page.extract_text() for page in reader.pages if page.extract_text())\n\n\n\neducation_schema = ResponseSchema(\n    name=\"education\",\n    description=\"A list of the candidate's education entries.\",\n)\n\nexperience_schema = ResponseSchema(\n    name=\"experience\",\n    description=\"A list of the candidate's work experience entries.\",\n)\n\n\nfull_name_schema = ResponseSchema(\n    name=\"full_name\",\n    description=\"The candidate's full name.\",\n)\n\nemail_schema = ResponseSchema(\n    name=\"email\",\n    description=\"The candidate's email address.\",\n)\n\nskills_schema = ResponseSchema(\n    name=\"skills\",\n    description=\"A list of the candidate's technical and soft skills.\",\n)\n\n\n\nresponse_schemas = [\n    full_name_schema,\n    email_schema,\n    education_schema,\n    skills_schema,\n    experience_schema\n]\noutput_parser = StructuredOutputParser.from_response_schemas(response_schemas)\nformat_instructions = output_parser.get_format_instructions()\n\npurchase_extraction_template = \"\"\"\nYou are a highly specialized HR assistant tasked with parsing raw resume text into a structured JSON format.\n\nExtract all relevant information from the resume text based *EXACTLY* on the provided JSON schema.\nIf a field is not present in the text, you must omit it or use an empty list/string as required by the schema.\n\nResume Text:\n\"{text}\"\n\nRespond ONLY in a JSON markdown code block, strictly adhering to the following structure:\n{format_instructions}\n\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_prompt = PromptTemplate(\n    template=purchase_extraction_template,\n    input_variables=[\"text\", \"format_instructions\"]\n).format(text=text, format_instructions=format_instructions)\n\nresponse = generate_text(final_prompt, max_length=5000)[0]\nprint(response)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_json_block(text):\n    pattern = r'```json\\s*(.*?)\\s*```'\n    matches = re.findall(pattern, text, re.DOTALL)\n\n    return f\"```json\\n{matches[-1]}\\n```\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"json_text = extract_json_block(response)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output_data = output_parser.parse(json_text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output_data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(type(output_data))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install streamlit pyngrok pillow python-docx pdfplumber streamlit-extras pytesseract","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile app.py\nimport streamlit as st\nimport base64\nimport pdfplumber\nimport json\nimport io\nfrom PIL import Image\nimport pytesseract\n\nst.set_page_config(\n    page_title=\"AI CV Parser\",\n    page_icon=\"üìÑ\",\n    layout=\"wide\"\n)\n\nst.markdown(\"\"\"\n<style>\n\nhtml, body, [class*=\"css\"]  {\n    font-family: 'Segoe UI', sans-serif;\n}\n\n.big-title {\n    font-size: 40px;\n    font-weight: 800;\n    text-align: center;\n    color: #2C3E50;\n}\n\n.subtitle {\n    font-size: 18px;\n    text-align: center;\n    color: #7F8C8D;\n    margin-bottom: 30px;\n}\n\n.upload-box {\n    border: 2px dashed #6C5CE7;\n    padding: 25px;\n    border-radius: 15px;\n    text-align: center;\n    background: #F8F9FF;\n}\n\n.result-card {\n    background: white;\n    padding: 20px;\n    border-radius: 15px;\n    box-shadow: 0 4px 14px rgba(0,0,0,0.1);\n    margin-bottom: 20px;\n}\n\n.section-title {\n    font-size: 22px;\n    font-weight: 700;\n    color: #6C5CE7;\n    margin-bottom: 10px;\n}\n\n</style>\n\"\"\", unsafe_allow_html=True)\n\nst.markdown(\"<div class='big-title'>üìÑ AI CV Parser</div>\", unsafe_allow_html=True)\nst.markdown(\"<div class='subtitle'>Upload your CV and extract structured information instantly</div>\", unsafe_allow_html=True)\n\n\n\nst.markdown(\"<div class='upload-box'>\", unsafe_allow_html=True)\nuploaded_file = st.file_uploader(\"Upload CV (PDF / PNG / JPG)\", type=[\"pdf\", \"png\", \"jpg\", \"jpeg\"])\nst.markdown(\"</div>\", unsafe_allow_html=True)\n\n\ndef extract_text_from_pdf(file_bytes):\n    text = \"\"\n    try:\n        with pdfplumber.open(io.BytesIO(file_bytes)) as pdf:\n            for page in pdf.pages:\n                text += page.extract_text() + \"\\n\"\n    except:\n        text = \"\"\n    return text\n\n\ndef extract_text_from_image(file_bytes):\n    image = Image.open(io.BytesIO(file_bytes))\n    return pytesseract.image_to_string(image)\n\n\ndef parse_cv_text(raw_text):\n\n\n    parsed = {\n        \"Name\": \"Unknown\",\n        \"Email\": \"Unknown\",\n        \"Phone\": \"Unknown\",\n        \"Skills\": [],\n        \"Experience\": []\n    }\n\n    lines = raw_text.split(\"\\n\")\n\n    for line in lines:\n        line_lower = line.lower()\n\n        if \"name\" in line_lower:\n            parsed[\"Name\"] = line.split(\":\")[-1].strip()\n\n        if \"email\" in line_lower or \"gmail\" in line_lower:\n            parsed[\"Email\"] = line.strip()\n\n        if \"phone\" in line_lower or \"+2\" in line_lower:\n            parsed[\"Phone\"] = line.strip()\n\n        if \"python\" in line_lower or \"java\" in line_lower or \"ml\" in line_lower:\n            parsed[\"Skills\"].append(line.strip())\n\n    parsed[\"Experience\"] = [\n        {\"Company\": \"ABC Corp\", \"Role\": \"Intern\", \"Years\": \"2023 - 2024\"}\n    ]\n\n    return parsed\n\n\nif uploaded_file:\n\n    st.success(\"File uploaded successfully!\")\n\n    if st.button(\"Parse CV\", use_container_width=True):\n\n        file_bytes = uploaded_file.read()\n\n        st.info(\"‚è≥ Extracting text...\")\n\n        if uploaded_file.type == \"application/pdf\":\n            raw_text = extract_text_from_pdf(file_bytes)\n        else:\n            raw_text = extract_text_from_image(file_bytes)\n\n        if not raw_text.strip():\n            st.error(\"‚ùå Could not extract text from the file. Try another CV.\")\n            st.stop()\n\n        st.info(\"‚è≥ Parsing CV details...\")\n\n        parsed = parse_cv_text(raw_text)\n\n        st.success(\"‚úÖ CV Parsed Successfully!\")\n\n        st.markdown(\"### üîç Extracted Information\")\n\n        st.markdown(\"<div class='result-card'>\", unsafe_allow_html=True)\n        st.markdown(\"<div class='section-title'>üë§ Personal Details</div>\", unsafe_allow_html=True)\n        st.write(f\"**Name:** {parsed['Name']}\")\n        st.write(f\"**Email:** {parsed['Email']}\")\n        st.write(f\"**Phone:** {parsed['Phone']}\")\n        st.markdown(\"</div>\", unsafe_allow_html=True)\n\n        st.markdown(\"<div class='result-card'>\", unsafe_allow_html=True)\n        st.markdown(\"<div class='section-title'>üõ† Skills</div>\", unsafe_allow_html=True)\n        st.write(\", \".join(parsed[\"Skills\"]))\n        st.markdown(\"</div>\", unsafe_allow_html=True)\n\n        st.markdown(\"<div class='result-card'>\", unsafe_allow_html=True)\n        st.markdown(\"<div class='section-title'>üíº Experience</div>\", unsafe_allow_html=True)\n        \n        for exp in parsed[\"Experience\"]:\n            st.write(f\"**{exp['Company']}** ‚Äî {exp['Role']} ({exp['Years']})\")\n\n        st.markdown(\"</div>\", unsafe_allow_html=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pyngrok streamlit\n\nfrom pyngrok import ngrok\nimport subprocess\nimport time\nimport os\n\nNGROK_AUTH_TOKEN = \"35t0D7y6l2yUBqAEjaI0nSNJVFk_67V8CLaVvQXr9ATPjpP5Y\"\nos.system(f\"ngrok config add-authtoken {NGROK_AUTH_TOKEN}\")\n\nngrok.kill()\n\nPORT = 8501\npublic_url = ngrok.connect(PORT)\nprint(\"üîó Your public URL:\", public_url)\n\nprocess = subprocess.Popen(\n    [\"streamlit\", \"run\", \"app.py\", \"--server.port\", str(PORT), \"--server.address=0.0.0.0\"]\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}